{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import networkx as nx\n",
    "from networkx.algorithms.centrality import betweenness_centrality\n",
    "\n",
    "from Datasets.synthetics import BA_2grid, BA_2grid_house, ProbingDataset, BA_2grid_to_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import networkx as nx\n",
    "from networkx.algorithms.centrality import betweenness_centrality\n",
    "\n",
    "import pickle as pkl\n",
    "from torch_geometric.utils import from_networkx\n",
    "import random\n",
    "\n",
    "from models.models_BA_2grid import GIN_framework as framework\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 1000)\n",
    "    # Pandas also uses np random state by default\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # if you are using GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(43)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Graph-Features Dataset\n",
    "\n",
    "The features for BA-2grid-house and ER-nb_stars2 datasets:\n",
    "- num_nodes\n",
    "- num_edges\n",
    "- density\n",
    "- average_shortest_path_length\n",
    "- transitivity\n",
    "- average_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_dataset = ProbingDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbingDataset(2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_dataset_to_test = BA_2grid_to_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.arange(len(gnn_dataset_to_test))\n",
    "train_idx, test_idx = train_test_split(idx, train_size=0.8,random_state=10)\n",
    "\n",
    "train_loader = DataLoader(gnn_dataset_to_test[train_idx],batch_size=256)\n",
    "test_loader = DataLoader(gnn_dataset_to_test[test_idx],batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of graphs in your dataset\n",
    "idx = torch.arange(len(probing_dataset))\n",
    "train_idx, test_idx = train_test_split(idx, train_size=0.8,random_state=10)\n",
    "\n",
    "probe_train_loader = DataLoader(probing_dataset[train_idx],batch_size=256)\n",
    "probe_test_loader = DataLoader(probing_dataset[test_idx],batch_size=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model to Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"GIN\"\n",
    "DATASET = \"BA_2grid\"\n",
    "dataset = BA_2grid()\n",
    "gnn = framework(dataset,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6930576267465091"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "gnn.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Probe, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_probe(probe, gnn, layer_idx):\n",
    "    gnn.model.eval()\n",
    "    probe.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(probe.parameters(), lr=0.001)\n",
    "\n",
    "    for graph in probe_train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = gnn.model(graph.x, graph.edge_index, graph.batch)\n",
    "        output = torch.sum(output, dim=1)\n",
    "        labels = graph.y.reshape(int(len(graph.y)/6), 6)\n",
    "        padded_output = torch.nn.functional.pad(output, (0, 256 - int(len(graph.y)/6)))\n",
    "        output = probe(padded_output)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += float(loss) * graph.num_graphs\n",
    "    return total_loss / len(probe_train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_probe(probe, test_loader):\n",
    "    probe.eval()\n",
    "\n",
    "    mse = 0\n",
    "    total = 0\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    for graph in test_loader:\n",
    "\n",
    "        output = gnn.model(graph.x, graph.edge_index, graph.batch)\n",
    "        output = torch.sum(output, dim=1)\n",
    "        y_true = graph.y.reshape(int(len(graph.y)/6), 6)\n",
    "        padded_output = torch.nn.functional.pad(output, (0, 256 - int(len(graph.y)/6)))\n",
    "        y_pred = probe(padded_output)\n",
    "\n",
    "        mse += loss_fn(y_pred, y_true)\n",
    "        total += 1\n",
    "    \n",
    "    return {\n",
    "        \"MSE\": mse/total\n",
    "        # \"R2 Score\": r2/total\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_to_train(gnn, layer_idx, num_epochs=10):\n",
    "    probe = Probe(256, 6)\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_probe(probe, gnn, layer_idx)\n",
    "        losses.append(loss)\n",
    "        result = evaluate_probe(probe, probe_test_loader)\n",
    "        test_acc = result['MSE']\n",
    "        test_losses.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, '\n",
    "              f'Test MSE: {test_acc:.3f}',\n",
    "              f'Train MSE: {loss:.3f}')\n",
    "        # print(f'Epoch: {epoch:03d}, '\n",
    "        #       f'Loss: {loss:.3f}, ')\n",
    "    return probe, losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([144, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Test MSE: 195.863 Train MSE: 211.692\n",
      "Epoch: 001, Test MSE: 159.280 Train MSE: 163.042\n",
      "Epoch: 002, Test MSE: 128.223 Train MSE: 122.930\n",
      "Epoch: 003, Test MSE: 101.302 Train MSE: 89.591\n",
      "Epoch: 004, Test MSE: 78.299 Train MSE: 62.055\n",
      "Epoch: 005, Test MSE: 59.278 Train MSE: 40.315\n",
      "Epoch: 006, Test MSE: 44.257 Train MSE: 24.368\n",
      "Epoch: 007, Test MSE: 33.299 Train MSE: 14.172\n",
      "Epoch: 008, Test MSE: 28.306 Train MSE: 9.554\n",
      "Epoch: 009, Test MSE: 27.332 Train MSE: 8.502\n",
      "Epoch: 010, Test MSE: 27.975 Train MSE: 8.497\n",
      "Epoch: 011, Test MSE: 27.130 Train MSE: 8.437\n",
      "Epoch: 012, Test MSE: 27.603 Train MSE: 8.425\n",
      "Epoch: 013, Test MSE: 26.872 Train MSE: 8.374\n",
      "Epoch: 014, Test MSE: 27.247 Train MSE: 8.353\n",
      "Epoch: 015, Test MSE: 26.584 Train MSE: 8.302\n",
      "Epoch: 016, Test MSE: 26.905 Train MSE: 8.285\n",
      "Epoch: 017, Test MSE: 26.282 Train MSE: 8.237\n",
      "Epoch: 018, Test MSE: 26.568 Train MSE: 8.214\n",
      "Epoch: 019, Test MSE: 25.975 Train MSE: 8.166\n",
      "Epoch: 020, Test MSE: 26.238 Train MSE: 8.147\n",
      "Epoch: 021, Test MSE: 25.666 Train MSE: 8.101\n",
      "Epoch: 022, Test MSE: 25.911 Train MSE: 8.077\n",
      "Epoch: 023, Test MSE: 25.358 Train MSE: 8.030\n",
      "Epoch: 024, Test MSE: 25.590 Train MSE: 8.011\n",
      "Epoch: 025, Test MSE: 25.049 Train MSE: 7.967\n",
      "Epoch: 026, Test MSE: 25.270 Train MSE: 7.942\n",
      "Epoch: 027, Test MSE: 24.742 Train MSE: 7.897\n",
      "Epoch: 028, Test MSE: 24.956 Train MSE: 7.877\n",
      "Epoch: 029, Test MSE: 24.437 Train MSE: 7.834\n",
      "Epoch: 030, Test MSE: 24.644 Train MSE: 7.809\n",
      "Epoch: 031, Test MSE: 24.134 Train MSE: 7.765\n",
      "Epoch: 032, Test MSE: 24.336 Train MSE: 7.746\n",
      "Epoch: 033, Test MSE: 23.832 Train MSE: 7.704\n",
      "Epoch: 034, Test MSE: 24.031 Train MSE: 7.681\n",
      "Epoch: 035, Test MSE: 23.533 Train MSE: 7.639\n",
      "Epoch: 036, Test MSE: 23.727 Train MSE: 7.614\n",
      "Epoch: 037, Test MSE: 23.237 Train MSE: 7.573\n",
      "Epoch: 038, Test MSE: 23.428 Train MSE: 7.551\n",
      "Epoch: 039, Test MSE: 22.943 Train MSE: 7.509\n",
      "Epoch: 040, Test MSE: 23.132 Train MSE: 7.490\n",
      "Epoch: 041, Test MSE: 22.651 Train MSE: 7.450\n",
      "Epoch: 042, Test MSE: 22.838 Train MSE: 7.427\n",
      "Epoch: 043, Test MSE: 22.362 Train MSE: 7.387\n",
      "Epoch: 044, Test MSE: 22.546 Train MSE: 7.363\n",
      "Epoch: 045, Test MSE: 22.075 Train MSE: 7.323\n",
      "Epoch: 046, Test MSE: 22.257 Train MSE: 7.302\n",
      "Epoch: 047, Test MSE: 21.791 Train MSE: 7.262\n",
      "Epoch: 048, Test MSE: 21.973 Train MSE: 7.243\n",
      "Epoch: 049, Test MSE: 21.510 Train MSE: 7.205\n",
      "Epoch: 050, Test MSE: 21.688 Train MSE: 7.180\n",
      "Epoch: 051, Test MSE: 21.231 Train MSE: 7.141\n",
      "Epoch: 052, Test MSE: 21.409 Train MSE: 7.123\n",
      "Epoch: 053, Test MSE: 20.954 Train MSE: 7.085\n",
      "Epoch: 054, Test MSE: 21.129 Train MSE: 7.061\n",
      "Epoch: 055, Test MSE: 20.681 Train MSE: 7.023\n",
      "Epoch: 056, Test MSE: 20.855 Train MSE: 7.004\n",
      "Epoch: 057, Test MSE: 20.409 Train MSE: 6.967\n",
      "Epoch: 058, Test MSE: 20.580 Train MSE: 6.943\n",
      "Epoch: 059, Test MSE: 20.140 Train MSE: 6.907\n",
      "Epoch: 060, Test MSE: 20.310 Train MSE: 6.888\n",
      "Epoch: 061, Test MSE: 19.873 Train MSE: 6.852\n",
      "Epoch: 062, Test MSE: 20.040 Train MSE: 6.828\n",
      "Epoch: 063, Test MSE: 19.608 Train MSE: 6.792\n",
      "Epoch: 064, Test MSE: 19.774 Train MSE: 6.771\n",
      "Epoch: 065, Test MSE: 19.347 Train MSE: 6.736\n",
      "Epoch: 066, Test MSE: 19.511 Train MSE: 6.717\n",
      "Epoch: 067, Test MSE: 19.087 Train MSE: 6.682\n",
      "Epoch: 068, Test MSE: 19.248 Train MSE: 6.658\n",
      "Epoch: 069, Test MSE: 18.829 Train MSE: 6.624\n",
      "Epoch: 070, Test MSE: 18.989 Train MSE: 6.603\n",
      "Epoch: 071, Test MSE: 18.574 Train MSE: 6.569\n",
      "Epoch: 072, Test MSE: 18.733 Train MSE: 6.551\n",
      "Epoch: 073, Test MSE: 18.321 Train MSE: 6.517\n",
      "Epoch: 074, Test MSE: 18.477 Train MSE: 6.493\n",
      "Epoch: 075, Test MSE: 18.071 Train MSE: 6.460\n",
      "Epoch: 076, Test MSE: 18.224 Train MSE: 6.439\n",
      "Epoch: 077, Test MSE: 17.822 Train MSE: 6.406\n",
      "Epoch: 078, Test MSE: 17.975 Train MSE: 6.388\n",
      "Epoch: 079, Test MSE: 17.576 Train MSE: 6.356\n",
      "Epoch: 080, Test MSE: 17.725 Train MSE: 6.332\n",
      "Epoch: 081, Test MSE: 17.332 Train MSE: 6.300\n",
      "Epoch: 082, Test MSE: 17.480 Train MSE: 6.283\n",
      "Epoch: 083, Test MSE: 17.090 Train MSE: 6.251\n",
      "Epoch: 084, Test MSE: 17.236 Train MSE: 6.227\n",
      "Epoch: 085, Test MSE: 16.852 Train MSE: 6.196\n",
      "Epoch: 086, Test MSE: 16.996 Train MSE: 6.175\n",
      "Epoch: 087, Test MSE: 16.618 Train MSE: 6.145\n",
      "Epoch: 088, Test MSE: 16.763 Train MSE: 6.126\n",
      "Epoch: 089, Test MSE: 16.389 Train MSE: 6.097\n",
      "Epoch: 090, Test MSE: 16.527 Train MSE: 6.081\n",
      "Epoch: 091, Test MSE: 16.155 Train MSE: 6.051\n",
      "Epoch: 092, Test MSE: 16.290 Train MSE: 6.027\n",
      "Epoch: 093, Test MSE: 15.923 Train MSE: 5.997\n",
      "Epoch: 094, Test MSE: 16.057 Train MSE: 5.980\n",
      "Epoch: 095, Test MSE: 15.693 Train MSE: 5.951\n",
      "Epoch: 096, Test MSE: 15.824 Train MSE: 5.927\n",
      "Epoch: 097, Test MSE: 15.466 Train MSE: 5.899\n",
      "Epoch: 098, Test MSE: 15.596 Train MSE: 5.882\n",
      "Epoch: 099, Test MSE: 15.240 Train MSE: 5.853\n",
      "Epoch: 100, Test MSE: 15.367 Train MSE: 5.829\n",
      "Epoch: 101, Test MSE: 15.017 Train MSE: 5.802\n",
      "Epoch: 102, Test MSE: 15.141 Train MSE: 5.781\n",
      "Epoch: 103, Test MSE: 14.795 Train MSE: 5.754\n",
      "Epoch: 104, Test MSE: 14.917 Train MSE: 5.734\n",
      "Epoch: 105, Test MSE: 14.576 Train MSE: 5.706\n",
      "Epoch: 106, Test MSE: 14.697 Train MSE: 5.690\n",
      "Epoch: 107, Test MSE: 14.359 Train MSE: 5.662\n",
      "Epoch: 108, Test MSE: 14.476 Train MSE: 5.639\n",
      "Epoch: 109, Test MSE: 14.143 Train MSE: 5.613\n",
      "Epoch: 110, Test MSE: 14.258 Train MSE: 5.593\n",
      "Epoch: 111, Test MSE: 13.930 Train MSE: 5.566\n",
      "Epoch: 112, Test MSE: 14.044 Train MSE: 5.550\n",
      "Epoch: 113, Test MSE: 13.719 Train MSE: 5.524\n",
      "Epoch: 114, Test MSE: 13.830 Train MSE: 5.501\n",
      "Epoch: 115, Test MSE: 13.510 Train MSE: 5.476\n",
      "Epoch: 116, Test MSE: 13.618 Train MSE: 5.456\n",
      "Epoch: 117, Test MSE: 13.303 Train MSE: 5.431\n",
      "Epoch: 118, Test MSE: 13.410 Train MSE: 5.415\n",
      "Epoch: 119, Test MSE: 13.098 Train MSE: 5.390\n",
      "Epoch: 120, Test MSE: 13.202 Train MSE: 5.367\n",
      "Epoch: 121, Test MSE: 12.895 Train MSE: 5.342\n",
      "Epoch: 122, Test MSE: 12.998 Train MSE: 5.327\n",
      "Epoch: 123, Test MSE: 12.694 Train MSE: 5.302\n",
      "Epoch: 124, Test MSE: 12.793 Train MSE: 5.279\n",
      "Epoch: 125, Test MSE: 12.495 Train MSE: 5.256\n",
      "Epoch: 126, Test MSE: 12.592 Train MSE: 5.237\n",
      "Epoch: 127, Test MSE: 12.298 Train MSE: 5.213\n",
      "Epoch: 128, Test MSE: 12.394 Train MSE: 5.198\n",
      "Epoch: 129, Test MSE: 12.103 Train MSE: 5.175\n",
      "Epoch: 130, Test MSE: 12.196 Train MSE: 5.152\n",
      "Epoch: 131, Test MSE: 11.910 Train MSE: 5.130\n",
      "Epoch: 132, Test MSE: 12.001 Train MSE: 5.110\n",
      "Epoch: 133, Test MSE: 11.720 Train MSE: 5.088\n",
      "Epoch: 134, Test MSE: 11.809 Train MSE: 5.073\n",
      "Epoch: 135, Test MSE: 11.531 Train MSE: 5.051\n",
      "Epoch: 136, Test MSE: 11.617 Train MSE: 5.028\n",
      "Epoch: 137, Test MSE: 11.344 Train MSE: 5.007\n",
      "Epoch: 138, Test MSE: 11.429 Train MSE: 4.988\n",
      "Epoch: 139, Test MSE: 11.160 Train MSE: 4.967\n",
      "Epoch: 140, Test MSE: 11.243 Train MSE: 4.952\n",
      "Epoch: 141, Test MSE: 10.977 Train MSE: 4.931\n",
      "Epoch: 142, Test MSE: 11.057 Train MSE: 4.908\n",
      "Epoch: 143, Test MSE: 10.796 Train MSE: 4.889\n",
      "Epoch: 144, Test MSE: 10.875 Train MSE: 4.869\n",
      "Epoch: 145, Test MSE: 10.618 Train MSE: 4.850\n",
      "Epoch: 146, Test MSE: 10.694 Train MSE: 4.830\n",
      "Epoch: 147, Test MSE: 10.441 Train MSE: 4.812\n",
      "Epoch: 148, Test MSE: 10.516 Train MSE: 4.792\n",
      "Epoch: 149, Test MSE: 10.267 Train MSE: 4.774\n",
      "Epoch: 150, Test MSE: 10.339 Train MSE: 4.755\n",
      "Epoch: 151, Test MSE: 10.094 Train MSE: 4.736\n",
      "Epoch: 152, Test MSE: 10.166 Train MSE: 4.722\n",
      "Epoch: 153, Test MSE: 9.924 Train MSE: 4.703\n",
      "Epoch: 154, Test MSE: 9.993 Train MSE: 4.681\n",
      "Epoch: 155, Test MSE: 9.755 Train MSE: 4.664\n",
      "Epoch: 156, Test MSE: 9.822 Train MSE: 4.644\n",
      "Epoch: 157, Test MSE: 9.589 Train MSE: 4.627\n",
      "Epoch: 158, Test MSE: 9.654 Train MSE: 4.608\n",
      "Epoch: 159, Test MSE: 9.425 Train MSE: 4.592\n",
      "Epoch: 160, Test MSE: 9.488 Train MSE: 4.573\n",
      "Epoch: 161, Test MSE: 9.263 Train MSE: 4.556\n",
      "Epoch: 162, Test MSE: 9.325 Train MSE: 4.543\n",
      "Epoch: 163, Test MSE: 9.102 Train MSE: 4.526\n",
      "Epoch: 164, Test MSE: 9.162 Train MSE: 4.503\n",
      "Epoch: 165, Test MSE: 8.944 Train MSE: 4.488\n",
      "Epoch: 166, Test MSE: 9.002 Train MSE: 4.469\n",
      "Epoch: 167, Test MSE: 8.788 Train MSE: 4.454\n",
      "Epoch: 168, Test MSE: 8.846 Train MSE: 4.441\n",
      "Epoch: 169, Test MSE: 8.634 Train MSE: 4.424\n",
      "Epoch: 170, Test MSE: 8.689 Train MSE: 4.402\n",
      "Epoch: 171, Test MSE: 8.482 Train MSE: 4.388\n",
      "Epoch: 172, Test MSE: 8.535 Train MSE: 4.369\n",
      "Epoch: 173, Test MSE: 8.332 Train MSE: 4.356\n",
      "Epoch: 174, Test MSE: 8.384 Train MSE: 4.337\n",
      "Epoch: 175, Test MSE: 8.184 Train MSE: 4.324\n",
      "Epoch: 176, Test MSE: 8.234 Train MSE: 4.305\n",
      "Epoch: 177, Test MSE: 8.038 Train MSE: 4.292\n",
      "Epoch: 178, Test MSE: 8.087 Train MSE: 4.273\n",
      "Epoch: 179, Test MSE: 7.895 Train MSE: 4.261\n",
      "Epoch: 180, Test MSE: 7.942 Train MSE: 4.242\n",
      "Epoch: 181, Test MSE: 7.753 Train MSE: 4.231\n",
      "Epoch: 182, Test MSE: 7.799 Train MSE: 4.211\n",
      "Epoch: 183, Test MSE: 7.615 Train MSE: 4.207\n",
      "Epoch: 184, Test MSE: 7.657 Train MSE: 4.185\n",
      "Epoch: 185, Test MSE: 7.476 Train MSE: 4.170\n",
      "Epoch: 186, Test MSE: 7.518 Train MSE: 4.152\n",
      "Epoch: 187, Test MSE: 7.340 Train MSE: 4.141\n",
      "Epoch: 188, Test MSE: 7.382 Train MSE: 4.123\n",
      "Epoch: 189, Test MSE: 7.207 Train MSE: 4.111\n",
      "Epoch: 190, Test MSE: 7.248 Train MSE: 4.101\n",
      "Epoch: 191, Test MSE: 7.075 Train MSE: 4.087\n",
      "Epoch: 192, Test MSE: 7.114 Train MSE: 4.066\n",
      "Epoch: 193, Test MSE: 6.946 Train MSE: 4.056\n",
      "Epoch: 194, Test MSE: 6.984 Train MSE: 4.038\n",
      "Epoch: 195, Test MSE: 6.819 Train MSE: 4.028\n",
      "Epoch: 196, Test MSE: 6.856 Train MSE: 4.010\n",
      "Epoch: 197, Test MSE: 6.695 Train MSE: 4.008\n",
      "Epoch: 198, Test MSE: 6.729 Train MSE: 3.988\n",
      "Epoch: 199, Test MSE: 6.571 Train MSE: 3.974\n"
     ]
    }
   ],
   "source": [
    "probe, losses, test_acc = iterate_to_train(gnn, 0, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efd8c0cf490>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = [i for i in range(200)]\n",
    "plt.plot(epochs, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.3374, grad_fn=<DivBackward0>)\n",
      "[-0.16237407 -0.49077247 -3.85665036 -0.09434725  0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "gnn.model.eval()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "mse = 0\n",
    "total = 0 \n",
    "r2_scores = []\n",
    "for graph in train_loader:\n",
    "    output = gnn.model(graph.x, graph.edge_index, graph.batch)\n",
    "    output = torch.sum(output, dim=1)\n",
    "    y_true = graph.y.reshape(int(len(graph.y)/6), 6)\n",
    "    padded_output = torch.nn.functional.pad(output, (0, 256 - int(len(graph.y)/6)))\n",
    "    y_pred = probe(padded_output)\n",
    "    \n",
    "    mse += loss_fn(y_pred, y_true)\n",
    "    total += 1\n",
    "\n",
    "    # Calculate R2 score per property\n",
    "    r2_per_property = []\n",
    "    y_true = y_true.T\n",
    "    for i in range(len(y_true)):\n",
    "        predictions = y_pred[i].detach().numpy().repeat(len(y_true[i]))\n",
    "        r2 = r2_score(y_true[i], predictions)\n",
    "        r2_per_property.append(r2)\n",
    "    r2_scores.append(r2_per_property)\n",
    "\n",
    "r2_scores = np.mean(r2_scores, axis=0)\n",
    "print(mse/total)\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.3374, grad_fn=<DivBackward0>)\n",
      "0.8735533281718862\n",
      "[0.8768088330778543, 0.8581255749276023, 0.8891247098610159, 0.8746931846967205, 0.8542179644032062, 0.8505672447727521, 0.9113357854640526]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([256, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/mara/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "gnn.model.eval()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "mse = 0\n",
    "total = 0 \n",
    "r2_scores = []\n",
    "for graph in train_loader:\n",
    "    output = gnn.model(graph.x, graph.edge_index, graph.batch)\n",
    "    output = torch.sum(output, dim=1)\n",
    "    y_true = graph.y.reshape(int(len(graph.y)/6), 6)\n",
    "    padded_output = torch.nn.functional.pad(output, (0, 256 - int(len(graph.y)/6)))\n",
    "    y_pred = probe(padded_output)\n",
    "    \n",
    "    mse += loss_fn(y_pred, y_true)\n",
    "    total += 1\n",
    "\n",
    "    # Calculate R2 score per property\n",
    "    r2_per_property = []\n",
    "    for i in range(len(y_true)):\n",
    "        r2 = r2_score(y_true[i], y_pred.detach().numpy())\n",
    "        r2_per_property.append(r2)\n",
    "    \n",
    "    r2_per_property = np.mean(r2_per_property)\n",
    "    r2_scores.append(r2_per_property)\n",
    "\n",
    "# r2_scores = np.mean(r2_scores, axis=0)\n",
    "print(mse/total)\n",
    "print(np.mean(r2_scores))\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
